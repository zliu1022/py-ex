Training with 1 chunks, validating on 1 chunks
Using 1 worker processes.
Using 1 worker processes.
gpus_num  0
x_planes (1, 18, 19, 19)
x_planes Tensor("fp32_storage/tower_0/transpose:0", shape=(1, 19, 19, 18), dtype=float32, device=/device:GPU:0)
x_planes (1, 19, 19, 18)
construct_net input: Tensor("fp32_storage/tower_0/transpose:0", shape=(1, 19, 19, 18), dtype=float32, device=/device:GPU:0)
        conv2d x (1, 19, 19, 18)
        conv2d W (3, 3, 18, 32)
construct_net input->conv: Tensor("fp32_storage/tower_0/Relu:0", shape=(1, 19, 19, 32), dtype=float32, device=/device:GPU:0)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
        conv2d x (1, 19, 19, 32)
        conv2d W (3, 3, 32, 32)
construct_net 4*resudial: Tensor("fp32_storage/tower_0/Relu_8:0", shape=(1, 19, 19, 32), dtype=float32, device=/device:GPU:0)
    conv_block input  Tensor("fp32_storage/tower_0/Relu_8:0", shape=(1, 19, 19, 32), dtype=float32, device=/device:GPU:0)
        conv2d x (1, 19, 19, 32)
        conv2d W (1, 1, 32, 2)
    conv_block conv2d  Tensor("fp32_storage/tower_0/Conv2D_9:0", shape=(1, 19, 19, 2), dtype=float32, device=/device:GPU:0)
    conv_block batch_norm  Tensor("fp32_storage/tower_0/bn9/batch_normalization/cond/Merge:0", shape=(1, 19, 19, 2), dtype=float32, device=/device:GPU:0)
    conv_block relu  Tensor("fp32_storage/tower_0/Relu_9:0", shape=(1, 19, 19, 2), dtype=float32, device=/device:GPU:0)
construct_net policy head: (1, 19, 19, 2)
construct_net policy head: (1, 2, 19, 19)
construct_net policy head: (1, 722)
construct_net h_fc1: (1, 362)
        conv2d x (1, 19, 19, 32)
        conv2d W (1, 1, 32, 1)
construct_net value head: (1, 19, 19, 1)
construct_net value head: (1, 19, 19, 1)
construct_net value head: (1, 19, 19, 1)
construct_net h_fc2: (1, 256)
construct_net h_fc3: (1, 1)

tower_loss
loss:         Tensor("fp32_storage/tower_0/add_7:0", shape=(), dtype=float32, device=/device:GPU:0)
policy_loss:  Tensor("fp32_storage/tower_0/Mean:0", shape=(), dtype=float32, device=/device:GPU:0)
mse_loss:     Tensor("fp32_storage/tower_0/Mean_1:0", shape=(), dtype=float32, device=/device:GPU:0)
reg_term:     Tensor("fp32_storage/tower_0/mul:0", shape=(), dtype=float32, device=/device:GPU:0)
y_conv:       Tensor("fp32_storage/tower_0/Add_4:0", shape=(1, 362), dtype=float32, device=/device:GPU:0)
z_conv:       Tensor("fp32_storage/tower_0/Tanh:0", shape=(1, 1), dtype=float32, device=/device:GPU:0)

Restoring from 4b32f-0
weight value check, len 50 training False
0 fp32_storage/first_conv:0 (3, 3, 18, 32) 4
1 fp32_storage/bn0/batch_normalization/beta:0 (32,) 1
2 fp32_storage/bn0/batch_normalization/moving_mean:0 (32,) 1
3 fp32_storage/bn0/batch_normalization/moving_variance:0 (32,) 1
4 fp32_storage/res_0_conv_1:0 (3, 3, 32, 32) 4
5 fp32_storage/bn1/batch_normalization/beta:0 (32,) 1
6 fp32_storage/bn1/batch_normalization/moving_mean:0 (32,) 1
7 fp32_storage/bn1/batch_normalization/moving_variance:0 (32,) 1
8 fp32_storage/res_0_conv_2:0 (3, 3, 32, 32) 4
9 fp32_storage/bn2/batch_normalization/beta:0 (32,) 1
10 fp32_storage/bn2/batch_normalization/moving_mean:0 (32,) 1
11 fp32_storage/bn2/batch_normalization/moving_variance:0 (32,) 1
12 fp32_storage/res_1_conv_1:0 (3, 3, 32, 32) 4
13 fp32_storage/bn3/batch_normalization/beta:0 (32,) 1
14 fp32_storage/bn3/batch_normalization/moving_mean:0 (32,) 1
15 fp32_storage/bn3/batch_normalization/moving_variance:0 (32,) 1
16 fp32_storage/res_1_conv_2:0 (3, 3, 32, 32) 4
17 fp32_storage/bn4/batch_normalization/beta:0 (32,) 1
18 fp32_storage/bn4/batch_normalization/moving_mean:0 (32,) 1
19 fp32_storage/bn4/batch_normalization/moving_variance:0 (32,) 1
20 fp32_storage/res_2_conv_1:0 (3, 3, 32, 32) 4
21 fp32_storage/bn5/batch_normalization/beta:0 (32,) 1
22 fp32_storage/bn5/batch_normalization/moving_mean:0 (32,) 1
23 fp32_storage/bn5/batch_normalization/moving_variance:0 (32,) 1
24 fp32_storage/res_2_conv_2:0 (3, 3, 32, 32) 4
25 fp32_storage/bn6/batch_normalization/beta:0 (32,) 1
26 fp32_storage/bn6/batch_normalization/moving_mean:0 (32,) 1
27 fp32_storage/bn6/batch_normalization/moving_variance:0 (32,) 1
28 fp32_storage/res_3_conv_1:0 (3, 3, 32, 32) 4
29 fp32_storage/bn7/batch_normalization/beta:0 (32,) 1
30 fp32_storage/bn7/batch_normalization/moving_mean:0 (32,) 1
31 fp32_storage/bn7/batch_normalization/moving_variance:0 (32,) 1
32 fp32_storage/res_3_conv_2:0 (3, 3, 32, 32) 4
33 fp32_storage/bn8/batch_normalization/beta:0 (32,) 1
34 fp32_storage/bn8/batch_normalization/moving_mean:0 (32,) 1
35 fp32_storage/bn8/batch_normalization/moving_variance:0 (32,) 1
36 fp32_storage/policy_head:0 (1, 1, 32, 2) 4
37 fp32_storage/bn9/batch_normalization/beta:0 (2,) 1
38 fp32_storage/bn9/batch_normalization/moving_mean:0 (2,) 1
39 fp32_storage/bn9/batch_normalization/moving_variance:0 (2,) 1
40 fp32_storage/w_fc_1:0 (722, 362) 2
41 fp32_storage/b_fc_1:0 (362,) 1
42 fp32_storage/value_head:0 (1, 1, 32, 1) 4
43 fp32_storage/bn10/batch_normalization/beta:0 (1,) 1
44 fp32_storage/bn10/batch_normalization/moving_mean:0 (1,) 1
45 fp32_storage/bn10/batch_normalization/moving_variance:0 (1,) 1
46 fp32_storage/w_fc_2:0 (361, 256) 2
47 fp32_storage/b_fc_2:0 (256,) 1
48 fp32_storage/w_fc_3:0 (256, 1) 2
49 fp32_storage/b_fc_3:0 (1,) 1

line 2  (3, 3, 18, 32)
-0.23941182 -0.18809368 -0.13790405 
-0.4078238 0.009355839 -0.20249508 
-0.35874325 -0.32546407 -0.24678291 

line 3 bias (32,)
-0.41782567 -0.55574405 -1.2150086 
line 4 mean (32,)
-1.2700325 -0.4345008 -0.64464414 
line 5 variance (32,)
1.0822941 0.5284246 0.4178641 
line 51 random -0.011889515
input
planes_len:   6498
probs_len:    1448
winner:       b'\x00\x00\x80?'

output 9
r[ 0 ] type:  <class 'numpy.float32'> size:  1 shape:  () ndim:  0 dtype:  float32
r[ 1 ] type:  <class 'numpy.float32'> size:  1 shape:  () ndim:  0 dtype:  float32
r[ 2 ] type:  <class 'numpy.float32'> size:  1 shape:  () ndim:  0 dtype:  float32
r[ 3 ] type:  <class 'numpy.float32'> size:  1 shape:  () ndim:  0 dtype:  float32
r[ 4 ] type:  <class 'numpy.ndarray'> size:  362 shape:  (1, 362) ndim:  2 dtype:  float32
r[ 5 ] type:  <class 'numpy.ndarray'> size:  1 shape:  (1, 1) ndim:  2 dtype:  float32
r[ 6 ] type:  <class 'numpy.ndarray'> size:  11552 shape:  (1, 19, 19, 32) ndim:  4 dtype:  float32
r[ 7 ] type:  <class 'numpy.ndarray'> size:  11552 shape:  (1, 19, 19, 32) ndim:  4 dtype:  float32
r[ 8 ] type:  <class 'numpy.ndarray'> size:  11552 shape:  (1, 19, 19, 32) ndim:  4 dtype:  float32

policy_loss:  1.6188949
mse_loss:     1.1933953
reg_term:     0.0583129
accuracy:     0.0
y_conv:       (1, 362)
-0.726 -0.613 -0.658 -0.673 -1.002 -0.874 -0.847 -0.953 -1.016 -0.892 -0.853 -0.821 -0.994 -0.819 -0.754 -0.654 -0.657 -0.786 -1.030 
-0.779 -0.688 -0.618 -0.202 -0.141 -1.138 -0.182 -0.563 -0.375 -0.416 -0.361 -0.467 -0.446 -1.104 -0.170 -0.068 -0.503 -0.382 -0.697 
-0.698 -0.540 +1.502 +3.649 +1.132 -0.490 +0.194 -0.100 +0.229 +0.385 +0.240 +0.054 +0.150 -0.559 +1.223 +3.936 +1.125 -0.105 -0.655 
-0.610 -0.498 +3.706 +6.466 +1.006 +0.055 +0.754 +0.454 +0.475 +0.484 +0.470 +0.606 +0.471 +0.398 +1.273 +6.651 +3.584 -0.454 -0.791 
-0.691 -0.311 +1.122 +1.483 -0.155 +0.061 -0.256 +0.085 -0.008 +0.033 +0.264 +0.219 +0.107 +0.180 +0.376 +1.201 +0.903 -0.099 -0.723 
-0.736 -1.429 -0.673 -0.767 +0.142 +0.259 +0.041 -0.031 +0.170 +0.205 +0.203 +0.277 +0.195 -0.048 +0.205 +0.304 -0.545 -1.270 -0.628 
-0.778 -0.824 +0.137 +0.352 -0.110 -0.043 +0.104 +0.094 +0.159 +0.223 +0.130 +0.203 +0.221 +0.193 +0.152 +0.156 +0.142 -0.444 -0.665 
-0.784 -0.411 -0.068 +0.114 +0.010 +0.075 +0.136 +0.050 +0.291 +0.158 +0.197 +0.248 +0.239 +0.207 +0.129 +0.238 -0.236 -0.425 -0.755 
-0.735 -0.327 +0.361 +0.195 -0.050 +0.119 +0.074 +0.076 +0.317 +0.281 +0.210 +0.220 +0.242 +0.173 +0.254 +0.245 +0.090 -0.409 -0.747 
-0.635 -0.609 +0.372 +0.273 -0.028 +0.046 +0.003 +0.077 +0.243 +0.180 +0.283 +0.171 +0.332 +0.121 +0.052 +0.404 +0.531 -0.180 -0.780 
-0.726 -0.309 +0.235 +0.191 +0.004 +0.138 +0.060 +0.064 +0.117 -0.045 +0.108 +0.319 +0.393 +0.250 +0.106 +0.127 +0.252 -0.342 -0.787 
-0.767 -0.695 +0.159 +0.151 -0.128 +0.049 -0.007 +0.026 +0.016 +0.037 +0.144 +0.111 +0.111 +0.072 -0.053 +0.215 -0.199 -0.516 -0.792 
-0.811 -0.873 +0.522 +0.199 -0.115 +0.194 -0.053 +0.000 +0.043 +0.237 +0.109 +0.182 +0.075 +0.141 +0.121 +0.399 -0.065 -0.490 -0.762 
-0.777 -1.351 -1.518 +0.181 -0.196 +0.116 -0.197 -0.172 +0.110 +0.039 +0.193 +0.042 +0.078 +0.044 +0.220 +0.047 -0.899 -1.063 -0.967 
-0.784 -1.045 +0.967 +1.539 -0.062 +0.094 -0.367 -0.154 -0.047 +0.064 -0.139 -0.241 -0.413 -0.118 +0.162 +1.198 +0.838 -0.705 -0.756 
-0.728 -0.718 +3.537 +6.422 +1.270 +0.166 +0.313 +0.055 +0.108 +0.117 -0.055 +0.169 +0.207 +0.391 +1.190 +6.565 +3.380 -0.079 -0.807 
-0.878 -0.359 +1.435 +3.279 +0.944 -0.973 -0.020 -0.241 -0.206 +0.141 -0.074 -0.063 -0.034 -0.756 +0.945 +3.459 +1.146 -0.410 -0.862 
-0.486 -0.569 -0.452 -0.241 -0.486 -1.192 -0.566 -0.716 -0.592 -0.667 -0.569 -0.575 -0.026 -1.091 -0.665 -0.148 -0.570 -0.740 -0.873 
-1.007 -0.469 -0.825 -0.894 -0.974 -0.849 -0.796 -1.053 -1.082 -1.035 -1.022 -1.007 -0.893 -0.882 -0.849 -0.867 -0.833 -0.787 -1.008 

  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   1  11   1   0   0   0   0   0   0   0   0   0   1  15   1   0   0 
  0   0  12 190   1   0   1   0   0   0   0   1   0   0   1 229  11   0   0 
  0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0 
  0   0  10 182   1   0   0   0   0   0   0   0   0   0   1 210   9   0   0 
  0   0   1   8   1   0   0   0   0   0   0   0   0   0   1   9   1   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
z_conv:      -0.092426285 -0.092690375 0.453521630

check bn...
bn0_beta -0.41782567 (32,)
bn0_mean -1.2700325 (32,)
bn0_var 1.0822941 (32,)

input before bn0
r[6] (1, 19, 19, 32)
arr  (19, 19)
-0.263 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.461 -0.184 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.464 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.868 -0.439 
-0.230 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.552 -0.306 

expect output after bn0, recalculate mean and var, but use input beta, training = True
new_mean -0.78764534
new_var  0.026706947
2.790 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 1.579 3.278 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
1.561 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 -0.908 1.717 
2.995 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 1.026 2.531 

expect output after bn0, use input mean and var and beta
0.550 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.626 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.582 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.509 

output after bn0
0.550 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.360 0.626 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.357 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 -0.031 0.381 
0.582 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.273 0.509 

losses:  {'policy': 1.6188949, 'mse': 0.2983488142490387, 'reg': 0.0583129, 'accuracy': 0.0, 'total': 2.870603}
break mandatory
